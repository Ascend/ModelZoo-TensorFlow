WARNING:tensorflow:From /usr/local/Ascend/tfplugin/latest/tfplugin/python/site-packages/npu_bridge/estimator/npu/npu_optimizer.py:127: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.

WARNING:tensorflow:From /home/test_user01/.local/lib/python3.7/site-packages/tensorpack/tfutils/common.py:148: The name tf.VERSION is deprecated. Please use tf.version.VERSION instead.

WARNING:tensorflow:
The TensorFlow contrib module will not be included in TensorFlow 2.0.
For more information, please see:
  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md
  * https://github.com/tensorflow/addons
  * https://github.com/tensorflow/io (for I/O related ops)
If you depend on functionality not listed there, please file an issue.

WARNING:tensorflow:From /home/test_user01/.local/lib/python3.7/site-packages/tensorpack/callbacks/graph.py:81: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.

WARNING:tensorflow:From /home/test_user01/.local/lib/python3.7/site-packages/tensorpack/callbacks/hooks.py:13: The name tf.train.SessionRunHook is deprecated. Please use tf.estimator.SessionRunHook instead.

WARNING:tensorflow:From /home/test_user01/.local/lib/python3.7/site-packages/tensorpack/tfutils/sesscreate.py:20: The name tf.train.ChiefSessionCreator is deprecated. Please use tf.compat.v1.train.ChiefSessionCreator instead.

WARNING:tensorflow:From /home/test_user01/.local/lib/python3.7/site-packages/tensorpack/tfutils/sesscreate.py:44: The name tf.train.SessionCreator is deprecated. Please use tf.compat.v1.train.SessionCreator instead.

WARNING:tensorflow:From train.py:43: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

WARNING:tensorflow:From train.py:45: The name tf.train.piecewise_constant is deprecated. Please use tf.compat.v1.train.piecewise_constant instead.

WARNING:tensorflow:From /home/test_user01/tsinghua_pcn/pcn_npu_20210610173506/models/pcn_cd.py:46: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.

WARNING:tensorflow:From /home/test_user01/tsinghua_pcn/pcn_npu_20210610173506/models/pcn_cd.py:46: The name tf.AUTO_REUSE is deprecated. Please use tf.compat.v1.AUTO_REUSE instead.

WARNING:tensorflow:From /usr/local/python3.7.5/lib/python3.7/site-packages/tensorflow_core/contrib/layers/python/layers/layers.py:1057: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `layer.__call__` method instead.
WARNING:tensorflow:From train.py:63: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.

WARNING:tensorflow:From train.py:66: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.

/home/test_user01/dataset/shapenet/train.lmdb
grid: [<tf.Tensor 'folding/meshgrid/mul:0' shape=(4, 4) dtype=float32>, <tf.Tensor 'folding/meshgrid/mul_1:0' shape=(4, 4) dtype=float32>]
chamfer: Tensor("decoder/Reshape:0", shape=(16, 1024, 3), dtype=float32) Tensor("ground_truths:0", shape=(16, 16384, 3), dtype=float32)
xyz1 Tensor("decoder/Reshape:0", shape=(16, 1024, 3), dtype=float32) xyz2 Tensor("ground_truths:0", shape=(16, 16384, 3), dtype=float32)
chamfer: Tensor("folding/add:0", shape=(16, 16384, 3), dtype=float32) Tensor("ground_truths:0", shape=(16, 16384, 3), dtype=float32)
xyz1 Tensor("folding/add:0", shape=(16, 16384, 3), dtype=float32) xyz2 Tensor("ground_truths:0", shape=(16, 16384, 3), dtype=float32)
[32m[0523 09:10:14 @format.py:92][0m Found 231792 entries in /home/test_user01/dataset/shapenet/train.lmdb
[32m[0523 09:10:14 @parallel.py:291][0m [PrefetchDataZMQ] Will fork a dataflow more than one times. This assumes the datapoints are i.i.d.
[32m[0523 09:10:14 @argtools.py:152][0m [5m[31mWRN[0m Install python-prctl so that processes can be cleaned with guarantee.
[32m[0523 09:10:14 @argtools.py:152][0m [5m[31mWRN[0m Install python-prctl so that processes can be cleaned with guarantee.
[32m[0523 09:10:14 @argtools.py:152][0m [5m[31mWRN[0m Install python-prctl so that processes can be cleaned with guarantee.
[32m[0523 09:10:14 @argtools.py:152][0m [5m[31mWRN[0m Install python-prctl so that processes can be cleaned with guarantee.
[32m[0523 09:10:14 @argtools.py:152][0m [5m[31mWRN[0m Install python-prctl so that processes can be cleaned with guarantee.
[32m[0523 09:10:14 @argtools.py:152][0m [5m[31mWRN[0m Install python-prctl so that processes can be cleaned with guarantee.
[32m[0523 09:10:14 @argtools.py:152][0m [5m[31mWRN[0m Install python-prctl so that processes can be cleaned with guarantee.
[32m[0523 09:10:14 @argtools.py:152][0m [5m[31mWRN[0m Install python-prctl so that processes can be cleaned with guarantee.
[32m[0523 09:10:14 @format.py:92][0m Found 800 entries in /home/test_user01/dataset/shapenet/valid.lmdb
WARNING:tensorflow:From train.py:80: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.

WARNING:tensorflow:From train.py:89: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.

2022-05-23 09:10:14.550748: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA
[32m[0523 09:10:14 @argtools.py:152][0m [5m[31mWRN[0m Install python-prctl so that processes can be cleaned with guarantee.
2022-05-23 09:10:14.591535: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2600000000 Hz
2022-05-23 09:10:14.596804: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x56353529ca10 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2022-05-23 09:10:14.596852: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2022-05-23 09:10:14.599417: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/Ascend/ascend-toolkit/latest/fwkacllib/lib64:/usr/lib:/usr/local/python3.7.5/lib:
2022-05-23 09:10:14.599448: E tensorflow/stream_executor/cuda/cuda_driver.cc:318] failed call to cuInit: UNKNOWN ERROR (303)
2022-05-23 09:10:14.599476: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (ubuntu): /proc/driver/nvidia/version does not exist
WARNING:tensorflow:From train.py:90: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.

WARNING:tensorflow:From train.py:96: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.

2022-05-23 09:10:14.901840: W /home/phisik3/jenkins/workspace/work_code/tmp/host-prefix/src/host-build/asl/tfadaptor/CMakeFiles/tf_adapter.dir/compiler_depend.ts:124] [GePlugin] can not find Environment variable : JOB_ID
2022-05-23 09:10:22.605594: I /home/phisik3/jenkins/workspace/work_code/tmp/host-prefix/src/host-build/asl/tfadaptor/CMakeFiles/tf_adapter.dir/compiler_depend.ts:777] The model has been compiled on the Ascend AI processor, current graph id is: 1
WARNING:tensorflow:From train.py:110: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.

2022-05-23 09:10:31.508551: I /home/phisik3/jenkins/workspace/work_code/tmp/host-prefix/src/host-build/asl/tfadaptor/CMakeFiles/tf_adapter.dir/compiler_depend.ts:777] The model has been compiled on the Ascend AI processor, current graph id is: 11
2022-05-23 09:10:32.053785: I /home/phisik3/jenkins/workspace/work_code/tmp/host-prefix/src/host-build/asl/tfadaptor/CMakeFiles/tf_adapter.dir/compiler_depend.ts:777] The model has been compiled on the Ascend AI processor, current graph id is: 21
2022-05-23 09:11:02.709703: I /home/phisik3/jenkins/workspace/work_code/tmp/host-prefix/src/host-build/asl/tfadaptor/CMakeFiles/tf_adapter.dir/compiler_depend.ts:777] The model has been compiled on the Ascend AI processor, current graph id is: 31
2022-05-23 09:11:03.863788: I /home/phisik3/jenkins/workspace/work_code/tmp/host-prefix/src/host-build/asl/tfadaptor/CMakeFiles/tf_adapter.dir/compiler_depend.ts:777] The model has been compiled on the Ascend AI processor, current graph id is: 41
epoch 1  step 1  loss 0.14132963 - time per batch 63.1364
epoch 1  step 2  loss 0.10214758 - time per batch 19.4204
epoch 1  step 3  loss 0.06877733 - time per batch 19.6910
epoch 1  step 4  loss 0.08079384 - time per batch 19.4275
epoch 1  step 5  loss 0.05800236 - time per batch 19.4076
epoch 1  step 6  loss 0.05725417 - time per batch 19.4098
epoch 1  step 7  loss 0.05703451 - time per batch 19.3252
epoch 1  step 8  loss 0.05358371 - time per batch 19.4569
epoch 1  step 9  loss 0.05957757 - time per batch 19.3906
epoch 1  step 10  loss 0.04725776 - time per batch 19.5508
epoch 1  step 11  loss 0.06461672 - time per batch 18.9922
epoch 1  step 12  loss 0.05718753 - time per batch 19.4453
epoch 1  step 13  loss 0.04927249 - time per batch 19.3812
epoch 1  step 14  loss 0.04766238 - time per batch 19.2331
epoch 1  step 15  loss 0.05868836 - time per batch 19.4302
epoch 1  step 16  loss 0.05412262 - time per batch 19.3263
epoch 1  step 17  loss 0.04975219 - time per batch 19.2885
epoch 1  step 18  loss 0.05473027 - time per batch 19.3011
epoch 1  step 19  loss 0.04507090 - time per batch 19.5038
epoch 1  step 20  loss 0.05985245 - time per batch 19.3276
epoch 1  step 21  loss 0.05978861 - time per batch 19.1754
epoch 1  step 22  loss 0.04666513 - time per batch 19.1892
epoch 1  step 23  loss 0.05112825 - time per batch 19.3484
epoch 1  step 24  loss 0.05368182 - time per batch 19.3035
epoch 1  step 25  loss 0.04803508 - time per batch 19.2635
epoch 1  step 26  loss 0.05142045 - time per batch 19.3427
epoch 1  step 27  loss 0.05123003 - time per batch 19.2810
epoch 1  step 28  loss 0.04960633 - time per batch 19.2791
epoch 1  step 29  loss 0.04720666 - time per batch 19.0440
epoch 1  step 30  loss 0.04400982 - time per batch 19.3178
epoch 1  step 31  loss 0.04706472 - time per batch 19.2932
epoch 1  step 32  loss 0.06146698 - time per batch 19.3136
epoch 1  step 33  loss 0.05635775 - time per batch 19.3391
epoch 1  step 34  loss 0.04746892 - time per batch 19.4138
epoch 1  step 35  loss 0.05777925 - time per batch 19.3084
epoch 1  step 36  loss 0.05535438 - time per batch 19.4136
epoch 1  step 37  loss 0.04434681 - time per batch 19.2375
epoch 1  step 38  loss 0.06020374 - time per batch 19.3251
epoch 1  step 39  loss 0.05856656 - time per batch 19.2736
epoch 1  step 40  loss 0.04437567 - time per batch 19.2604
epoch 1  step 41  loss 0.04306724 - time per batch 19.3049
epoch 1  step 42  loss 0.04677003 - time per batch 19.3281
epoch 1  step 43  loss 0.05887580 - time per batch 19.2858
epoch 1  step 44  loss 0.05283421 - time per batch 19.3170
epoch 1  step 45  loss 0.05749103 - time per batch 19.4523
epoch 1  step 46  loss 0.04926259 - time per batch 19.3507
epoch 1  step 47  loss 0.05038521 - time per batch 19.2456
epoch 1  step 48  loss 0.05630658 - time per batch 19.2798
epoch 1  step 49  loss 0.04621692 - time per batch 19.4373
epoch 1  step 50  loss 0.05538236 - time per batch 19.2820
epoch 1  step 51  loss 0.06471530 - time per batch 19.1569
epoch 1  step 52  loss 0.04023525 - time per batch 19.2711
epoch 1  step 53  loss 0.04816820 - time per batch 19.2107
epoch 1  step 54  loss 0.05273956 - time per batch 19.3186
epoch 1  step 55  loss 0.04873675 - time per batch 19.2475
epoch 1  step 56  loss 0.05415042 - time per batch 19.2747
epoch 1  step 57  loss 0.04955344 - time per batch 19.2537
epoch 1  step 58  loss 0.04894833 - time per batch 19.3620
epoch 1  step 59  loss 0.04957427 - time per batch 19.4263
epoch 1  step 60  loss 0.04596641 - time per batch 19.3185
epoch 1  step 61  loss 0.05236939 - time per batch 19.2589
epoch 1  step 62  loss 0.05864403 - time per batch 19.5652
epoch 1  step 63  loss 0.04879878 - time per batch 19.2439
epoch 1  step 64  loss 0.04064313 - time per batch 19.1425
epoch 1  step 65  loss 0.04744138 - time per batch 19.3059
epoch 1  step 66  loss 0.05520155 - time per batch 19.5880
epoch 1  step 67  loss 0.04658900 - time per batch 19.1599
epoch 1  step 68  loss 0.04806674 - time per batch 19.2746
epoch 1  step 69  loss 0.05207698 - time per batch 19.3179
epoch 1  step 70  loss 0.04553970 - time per batch 19.2772
epoch 1  step 71  loss 0.04329488 - time per batch 19.3373
epoch 1  step 72  loss 0.05312859 - time per batch 19.3817
epoch 1  step 73  loss 0.03624860 - time per batch 19.3126
epoch 1  step 74  loss 0.04357535 - time per batch 19.2626
epoch 1  step 75  loss 0.04474372 - time per batch 19.2916
epoch 1  step 76  loss 0.04311376 - time per batch 19.3068
epoch 1  step 77  loss 0.04253598 - time per batch 19.2797
epoch 1  step 78  loss 0.05014879 - time per batch 19.3053
epoch 1  step 79  loss 0.04135763 - time per batch 19.3497
epoch 1  step 80  loss 0.04439468 - time per batch 19.3724
epoch 1  step 81  loss 0.04202792 - time per batch 19.2550
epoch 1  step 82  loss 0.03818765 - time per batch 19.2590
epoch 1  step 83  loss 0.04008320 - time per batch 19.2391
epoch 1  step 84  loss 0.04526770 - time per batch 19.3320
epoch 1  step 85  loss 0.04184875 - time per batch 19.2132
epoch 1  step 86  loss 0.04435479 - time per batch 19.2735
epoch 1  step 87  loss 0.04017988 - time per batch 19.2089
epoch 1  step 88  loss 0.03510582 - time per batch 19.3731
epoch 1  step 89  loss 0.03330578 - time per batch 19.0095
epoch 1  step 90  loss 0.04026211 - time per batch 19.2894
epoch 1  step 91  loss 0.03936208 - time per batch 19.6393
epoch 1  step 92  loss 0.03735173 - time per batch 19.5225
epoch 1  step 93  loss 0.03478570 - time per batch 19.3138
epoch 1  step 94  loss 0.05250512 - time per batch 19.3104
epoch 1  step 95  loss 0.03974887 - time per batch 19.3324
epoch 1  step 96  loss 0.04829241 - time per batch 19.2950
epoch 1  step 97  loss 0.04069317 - time per batch 19.3095
epoch 1  step 98  loss 0.03997764 - time per batch 19.2686
epoch 1  step 99  loss 0.03774005 - time per batch 19.4254
epoch 1  step 100  loss 0.03590576 - time per batch 19.3322
epoch 1  step 101  loss 0.03617168 - time per batch 19.3991
epoch 1  step 102  loss 0.03623088 - time per batch 19.2935
epoch 1  step 103  loss 0.03443495 - time per batch 19.2370
epoch 1  step 104  loss 0.04326998 - time per batch 19.3402
epoch 1  step 105  loss 0.03378446 - time per batch 19.1973
epoch 1  step 106  loss 0.03696116 - time per batch 19.3637
epoch 1  step 107  loss 0.03805485 - time per batch 18.9816
epoch 1  step 108  loss 0.03155085 - time per batch 19.1692
epoch 1  step 109  loss 0.04232705 - time per batch 19.1422
epoch 1  step 110  loss 0.03894557 - time per batch 19.1689
epoch 1  step 111  loss 0.04081194 - time per batch 19.3352
epoch 1  step 112  loss 0.03204824 - time per batch 19.0387
epoch 1  step 113  loss 0.03802953 - time per batch 19.0611
epoch 1  step 114  loss 0.04442226 - time per batch 19.1594
epoch 1  step 115  loss 0.03552597 - time per batch 19.2200
epoch 1  step 116  loss 0.04328145 - time per batch 19.1857
epoch 1  step 117  loss 0.03751010 - time per batch 19.2655
epoch 1  step 118  loss 0.03721461 - time per batch 19.0242
epoch 1  step 119  loss 0.03453793 - time per batch 19.1745
epoch 1  step 120  loss 0.04380044 - time per batch 19.0015
epoch 1  step 121  loss 0.03330122 - time per batch 18.9725
epoch 1  step 122  loss 0.03525963 - time per batch 19.2226
epoch 1  step 123  loss 0.03533386 - time per batch 19.2678
epoch 1  step 124  loss 0.03822278 - time per batch 19.2170
epoch 1  step 125  loss 0.03171182 - time per batch 19.3288
epoch 1  step 126  loss 0.03329223 - time per batch 19.2333
epoch 1  step 127  loss 0.03167506 - time per batch 19.2764
epoch 1  step 128  loss 0.03674341 - time per batch 19.2797
epoch 1  step 129  loss 0.04507549 - time per batch 19.3328
epoch 1  step 130  loss 0.03096771 - time per batch 19.1991
epoch 1  step 131  loss 0.03204722 - time per batch 19.2405
epoch 1  step 132  loss 0.03422359 - time per batch 19.3383
epoch 1  step 133  loss 0.03707635 - time per batch 19.2423
epoch 1  step 134  loss 0.03927414 - time per batch 19.1714
epoch 1  step 135  loss 0.03468872 - time per batch 19.2563
epoch 1  step 136  loss 0.03987839 - time per batch 19.2616
epoch 1  step 137  loss 0.03744692 - time per batch 19.1657
epoch 1  step 138  loss 0.03970168 - time per batch 19.1255
epoch 1  step 139  loss 0.03808723 - time per batch 19.2446
epoch 1  step 140  loss 0.03219742 - time per batch 19.2943
epoch 1  step 141  loss 0.03468522 - time per batch 19.1289
epoch 1  step 142  loss 0.03579857 - time per batch 19.2103
epoch 1  step 143  loss 0.03691458 - time per batch 19.1897
epoch 1  step 144  loss 0.02979552 - time per batch 19.2459
epoch 1  step 145  loss 0.03886828 - time per batch 19.0320
epoch 1  step 146  loss 0.03241957 - time per batch 19.1952
epoch 1  step 147  loss 0.03705544 - time per batch 19.1805
epoch 1  step 148  loss 0.03430630 - time per batch 19.2598
epoch 1  step 149  loss 0.02940665 - time per batch 19.1983
epoch 1  step 150  loss 0.03068841 - time per batch 18.9740
epoch 1  step 151  loss 0.03510536 - time per batch 19.2151
epoch 1  step 152  loss 0.04151751 - time per batch 19.2337
epoch 1  step 153  loss 0.03815142 - time per batch 19.1765
epoch 1  step 154  loss 0.03279383 - time per batch 19.1424
epoch 1  step 155  loss 0.03556178 - time per batch 19.3001
epoch 1  step 156  loss 0.03446425 - time per batch 19.2039
epoch 1  step 157  loss 0.02418816 - time per batch 19.2631
epoch 1  step 158  loss 0.03062450 - time per batch 19.1728
epoch 1  step 159  loss 0.03376612 - time per batch 19.2065
epoch 1  step 160  loss 0.03272252 - time per batch 19.2766
epoch 1  step 161  loss 0.03346011 - time per batch 19.3273
epoch 1  step 162  loss 0.03448610 - time per batch 19.1760
epoch 1  step 163  loss 0.03461931 - time per batch 19.3290
epoch 1  step 164  loss 0.03144133 - time per batch 19.3096
epoch 1  step 165  loss 0.03130235 - time per batch 19.3358
epoch 1  step 166  loss 0.04320062 - time per batch 19.4125
epoch 1  step 167  loss 0.02988959 - time per batch 19.0411
epoch 1  step 168  loss 0.03293911 - time per batch 19.2691
epoch 1  step 169  loss 0.03405322 - time per batch 19.2557
epoch 1  step 170  loss 0.02903248 - time per batch 19.2221
epoch 1  step 171  loss 0.03483143 - time per batch 19.1787
epoch 1  step 172  loss 0.03227855 - time per batch 19.3139
epoch 1  step 173  loss 0.03162795 - time per batch 19.5197
epoch 1  step 174  loss 0.03241986 - time per batch 19.1496
epoch 1  step 175  loss 0.03866060 - time per batch 19.2322
epoch 1  step 176  loss 0.04368113 - time per batch 19.1649
epoch 1  step 177  loss 0.03618311 - time per batch 19.2903
epoch 1  step 178  loss 0.03603082 - time per batch 19.2576
epoch 1  step 179  loss 0.03538582 - time per batch 19.3655
epoch 1  step 180  loss 0.04303199 - time per batch 19.3344
epoch 1  step 181  loss 0.04473616 - time per batch 19.2244
epoch 1  step 182  loss 0.03505236 - time per batch 19.2479
epoch 1  step 183  loss 0.03878648 - time per batch 19.1541
epoch 1  step 184  loss 0.03392501 - time per batch 19.2836
epoch 1  step 185  loss 0.03643661 - time per batch 19.3267
epoch 1  step 186  loss 0.03718861 - time per batch 19.1656
epoch 1  step 187  loss 0.03231688 - time per batch 19.2569
epoch 1  step 188  loss 0.03873545 - time per batch 18.9537
epoch 1  step 189  loss 0.02746708 - time per batch 19.2054
epoch 1  step 190  loss 0.02947619 - time per batch 19.1980
epoch 1  step 191  loss 0.03722466 - time per batch 19.2067
epoch 1  step 192  loss 0.02890562 - time per batch 19.2832
epoch 1  step 193  loss 0.03481714 - time per batch 19.2629
epoch 1  step 194  loss 0.03377806 - time per batch 19.3331
epoch 1  step 195  loss 0.03316282 - time per batch 19.1889
epoch 1  step 196  loss 0.02477519 - time per batch 19.5113
epoch 1  step 197  loss 0.03427351 - time per batch 19.2787
epoch 1  step 198  loss 0.03072201 - time per batch 19.2367
epoch 1  step 199  loss 0.03725777 - time per batch 19.1236
epoch 1  step 200  loss 0.03277017 - time per batch 19.2561
epoch 1  step 201  loss 0.03687967 - time per batch 19.1752
epoch 1  step 202  loss 0.03097342 - time per batch 19.2436
epoch 1  step 203  loss 0.02829147 - time per batch 19.0099
epoch 1  step 204  loss 0.03180674 - time per batch 19.2614
epoch 1  step 205  loss 0.02578442 - time per batch 19.1944
epoch 1  step 206  loss 0.02786331 - time per batch 19.1898
epoch 1  step 207  loss 0.03404994 - time per batch 19.1841
epoch 1  step 208  loss 0.03687341 - time per batch 18.9217
epoch 1  step 209  loss 0.02694423 - time per batch 19.2094
epoch 1  step 210  loss 0.03475713 - time per batch 19.2886
epoch 1  step 211  loss 0.03371142 - time per batch 19.5478
epoch 1  step 212  loss 0.03717136 - time per batch 19.0453
epoch 1  step 213  loss 0.02531748 - time per batch 19.0591
epoch 1  step 214  loss 0.02961834 - time per batch 19.1618
epoch 1  step 215  loss 0.03220876 - time per batch 19.0078
epoch 1  step 216  loss 0.02938957 - time per batch 19.0615
epoch 1  step 217  loss 0.02711295 - time per batch 19.2724
epoch 1  step 218  loss 0.03222395 - time per batch 19.2934
epoch 1  step 219  loss 0.03546574 - time per batch 18.9217
epoch 1  step 220  loss 0.03144527 - time per batch 19.1634
epoch 1  step 221  loss 0.03447419 - time per batch 19.1861
epoch 1  step 222  loss 0.02811983 - time per batch 19.1480
epoch 1  step 223  loss 0.03429939 - time per batch 19.2146
epoch 1  step 224  loss 0.03294478 - time per batch 19.1811
epoch 1  step 225  loss 0.02905173 - time per batch 19.2097
epoch 1  step 226  loss 0.02872484 - time per batch 19.1820
epoch 1  step 227  loss 0.02752861 - time per batch 18.9728
epoch 1  step 228  loss 0.03022791 - time per batch 19.2102
epoch 1  step 229  loss 0.02864184 - time per batch 19.1623
epoch 1  step 230  loss 0.02584100 - time per batch 19.2409
epoch 1  step 231  loss 0.03006678 - time per batch 19.1974
epoch 1  step 232  loss 0.03002537 - time per batch 19.3340
epoch 1  step 233  loss 0.02604391 - time per batch 19.2297
epoch 1  step 234  loss 0.02900777 - time per batch 18.8816
epoch 1  step 235  loss 0.03071318 - time per batch 19.2449
epoch 1  step 236  loss 0.02774770 - time per batch 19.1436
epoch 1  step 237  loss 0.02855639 - time per batch 19.3212
epoch 1  step 238  loss 0.02722044 - time per batch 19.2018
epoch 1  step 239  loss 0.02974634 - time per batch 19.1759
epoch 1  step 240  loss 0.02486368 - time per batch 19.0048
epoch 1  step 241  loss 0.02695630 - time per batch 19.0533
epoch 1  step 242  loss 0.03186243 - time per batch 19.2399
epoch 1  step 243  loss 0.02679856 - time per batch 19.0401
epoch 1  step 244  loss 0.02771508 - time per batch 19.2300
epoch 1  step 245  loss 0.03197013 - time per batch 19.4960
epoch 1  step 246  loss 0.02781197 - time per batch 19.1088
epoch 1  step 247  loss 0.02565728 - time per batch 19.0688
epoch 1  step 248  loss 0.02939209 - time per batch 19.5411
epoch 1  step 249  loss 0.02846311 - time per batch 19.3024
epoch 1  step 250  loss 0.02558803 - time per batch 19.1071
epoch 1  step 251  loss 0.02865807 - time per batch 19.4655
epoch 1  step 252  loss 0.03057484 - time per batch 19.2351
epoch 1  step 253  loss 0.02706343 - time per batch 19.2224
epoch 1  step 254  loss 0.02800732 - time per batch 19.2040
epoch 1  step 255  loss 0.02830721 - time per batch 19.2173
epoch 1  step 256  loss 0.03461130 - time per batch 19.2211
epoch 1  step 257  loss 0.02927726 - time per batch 19.2382
epoch 1  step 258  loss 0.03335039 - time per batch 19.0982
epoch 1  step 259  loss 0.02828859 - time per batch 19.0933
epoch 1  step 260  loss 0.03432784 - time per batch 19.2164
epoch 1  step 261  loss 0.02988109 - time per batch 19.3456
epoch 1  step 262  loss 0.02902311 - time per batch 19.3398
epoch 1  step 263  loss 0.02766091 - time per batch 19.4597
epoch 1  step 264  loss 0.02764358 - time per batch 19.2774
epoch 1  step 265  loss 0.02788440 - time per batch 19.2081
epoch 1  step 266  loss 0.02672531 - time per batch 19.3073
epoch 1  step 267  loss 0.03163437 - time per batch 19.1833
epoch 1  step 268  loss 0.02749340 - time per batch 18.9700
epoch 1  step 269  loss 0.03129300 - time per batch 19.4256
epoch 1  step 270  loss 0.03450108 - time per batch 19.2435
epoch 1  step 271  loss 0.02368694 - time per batch 19.1818
epoch 1  step 272  loss 0.02663878 - time per batch 19.1695
epoch 1  step 273  loss 0.03526736 - time per batch 19.2367
epoch 1  step 274  loss 0.03323716 - time per batch 19.6031
epoch 1  step 275  loss 0.02640418 - time per batch 19.4206
epoch 1  step 276  loss 0.02740633 - time per batch 19.2787